---
title: "GLMNET"
format: revealjs
editor: visual
---

## Problems

-   You have more variables than observations e.g. genomic data
-   You have multi-collinearity but don't know which ones to remove
-   You want to discover possible linear models
-   You want to rank variables based on effect size, not arbitrary numbers

## **Terms...**

-   **Probability:** **P(data‚à£parameters)** - Used to predict from a model

-   **Likelihood:** **L(parameters‚à£data)** - used to find the best model from data.

-   Likelihood is the product of the probabilities of an independent set of events.

-   In Maximum Likelihood Estimation, we maximize the log-likelihood **‚Ñì(Œ∏)**

-   Optimization techniques are used to quickly find the MLE parameters

-   **Overfitting Concern:** Complex models or limited data can lead to parameter estimates that fit the training data too well, sacrificing generalizability

## Penalized Maximum Likelihood

-   An extension of the standard maximum likelihood estimation (MLE) framework

-   **Likelihood Function:** In MLE, we maximize the log-likelihood ‚Ñì(Œ∏) for parameters Œ∏ given data y

-   **Addressing overfitting:** We add a penalty term P(Œ∏) to the log-likelihood, which imposes a cost on extreme or overly complex parameter values. We can also scale up the penalty by multiplying it with a Œª term

-   So our objective becomes tp maximize ‚Ñì(Œ∏)‚àíŒªP(Œ∏)

-   Œ∏ is our parameter set. It is what we want to fit

## The Penalty term

### ùêø‚ÇÇ Penalty

The ùêø‚ÇÇ penalty adds a term to the model‚Äôs loss (or cost) function that is proportional to the sum of the squares of the model‚Äôs weights. In regression, this is called "ridge regularization"

In least squares regression, the Sum of Squared Errors (SSE) is defined as:

$$
\text{SSE} = \sum_{i=1}^{n} \left(y_i - \hat{y}_i\right)^2.
$$

Conceptually, when you add an (L2) penalty (ridge regularization), the loss function could become:

$$
J(\mathbf{w}) = \text{SSE} + \lambda \sum_{j=1}^{p} w_j^2,
$$

where: - w are the weights of the model, lambda is the regularization parameter controlling the strength of the penalty.

Usually we use the ML function instead of the traditional SSE

**Benefits:**

-   The penalty term increases the loss for any models with large weights. This encourages the optimization algorithm to keep the weights small.
-   The final model after optimisation will have weights that are less sensitive to the noise in the training data, which typically leads to better generalization on unseen data.

### L1 Penalty

Lasso regression (Least Absolute Shrinkage and Selection Operator) is a technique that performs both variable selection and regularization, which enhances the prediction accuracy and interpretability of statistical models.

### Objective Function

using MSE as an example, Lasso regression adds an (L1) penalty term to the loss function:

$$
J(\mathbf{w}) = \text{MSE} + \lambda \sum_{j=1}^{p} |w_j|,
$$

where: \|w_j\| is the (L1) norm of the coefficients.

### Effects of Lasso

-   **Feature Selection:**\
    The (L1) penalty has the property of forcing some coefficients to be exactly zero when lambda is sufficiently large. This effectively selects a subset of the original features, leading to a simpler model.

-   **Model Complexity:**\
    A larger lambda enforces stronger regularization, leading to a model with fewer non-zero coefficients, while a smaller lambda keeps the model closer to standard linear regression.

### Practical Considerations

-   **Standardization:**\
    It is common practice to standardize predictors before applying lasso regression so that the (L1) penalty is applied uniformly across all coefficients.

## Why do we care?

### Ridge

-   helps us find the best set of regression coefficients without overfitting.
-   Every variable will have a coefficient, but the least important variables will have very low coefficients close to Zero
-   can be used to rank our variables

### Lasso

-   aggressively shrinks unimportant coefficients to Zero and eliminates them from the model entirely
-   If you have loads of variables without theoretical basis for removing some, Lasso will demote them for you
-   if you have multicollinearity, lasso will eliminate the redundant variables

## What if both Lasso and Ridge are attractive?

### Elastic Net

-   It is essentially a hybrid approach that blends the properties of both lasso (L1) and ridge (L2) regularizations.
-   It incorporates both penalties into the loss function, allowing you to balance between feature selection (lasso) and coefficient shrinkage (ridge)

$$
J(\mathbf{w}) = \text{MSE} + \lambda \left( \alpha \sum_{j=1}^{p} |w_j| + \frac{1 - \alpha}{2} \sum_{j=1}^{p} w_j^2 \right)
$$

-   It's essentially the ridge and lasso penalties added together, plus an alpha parameter that determines which of the two penalties is stronger
-   You end up with some variables eliminated, some shrunk towards zero.

## R implementation

-   Glmnet is a package that fits generalized linear and similar models via penalized maximum likelihood.
-   You can essentially have an ElasticNet model and tune lambda and alpha and train your models. An alpha of 1 equals Lasso regression, Alpha of 0 equals Ridge regression, anything in the middle is a proportional mixture of the two
-   Personally, i use the CARET package to fine tune my hyperparameters

```{r}
# Load necessary libraries
library(caret)
library(glmnet)

# Load the data
data <- read.csv("orf_2011_220225.csv")
y<-as.factor(data$Orf.yes.no)
data<-na.omit(data[,c(7:21)])
data[, sapply(data, is.numeric)] <- scale(data[, sapply(data, is.numeric)])
data$yvar<-y

set.seed(123)

```

Data partitioning

```{r}
# Partition the data into training (80%) and testing (20%) sets
trainIndex <- createDataPartition(data$yvar, p = 0.8, list = FALSE)
trainData <- data[trainIndex, ]
testData  <- data[-trainIndex, ]

```

Setup the training parameter space and cross validation settings

```{r}
# Define cross-validation method (10-fold CV)
trainControlObj <- trainControl(method = "cv", number = 10)

# Set up a grid of lambda values for tuning; alpha is fixed to 0 for ridge regression
ridgeGrid <- expand.grid(alpha = 1, 
                         lambda = seq(0.001, 1, length = 100))

```

Now train the model

```{r}
# Train the ridge regression model using caret's train() function with the 'glmnet' method
set.seed(123)
ridgeModel <- train(yvar ~ ., 
                    data = trainData, 
                    method = "glmnet", 
                    trControl = trainControlObj, 
                    tuneGrid = ridgeGrid)

# Print the trained model and tuning results
print(ridgeModel)

```

Predict

```{r}

# Make predictions on the test set
predictions <- predict(ridgeModel, newdata = testData)
confusionMatrix(predictions,testData$yvar)
```

Draw the confmat

```{r}
# Load necessary libraries
library(caret)
library(ggplot2)
library(reshape2)  # for melting the confusion matrix table

# Generate the confusion matrix using caret's confusionMatrix function
confmat <- confusionMatrix(predictions, testData$yvar)

# Print the confusion matrix object
print(confmat)

# Convert the table from the confusion matrix into a data frame for ggplot
cm_df <- as.data.frame(confmat$table)

# Plot the confusion matrix as a heatmap
ggplot(cm_df, aes(x = Prediction, y = Reference, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), color = "white", size = 6) +
  scale_fill_gradient(low = "blue", high = "red") +
  labs(title = "Confusion Matrix", x = "Predicted", y = "Actual") +
  theme_minimal() +
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 14, face = "bold"),
        plot.title = element_text(size = 16, face = "bold", hjust = 0.5))
7
```

```{r}
varImp(ridgeModel)
```

Model

```{r}
# Extract the coefficients for the best lambda value selected during training
best_lambda <- ridgeModel$bestTune$lambda
coef_values <- coef(ridgeModel$finalModel, s = best_lambda)
print(coef_values)
```

The coef_values are your regression coefficients. Some are reduced to 0 and eliminated from the model by Lasso. You can change alpha to 0 and retrain the model to try out ridge regression and see results.
